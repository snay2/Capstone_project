---
title: "SwiftKey Research Milestone Report"
author: "Steve Nay"
date: "March 16, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(echo=FALSE)
knitr::opts_chunk$set(include=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(warnings=FALSE)
options(scipen=50) # Prefer not using scientific notation
```

# Introduction
In this report, we will examine a random subset of the provided English corpus to learn about the features of the dataset.

# Summarizing the data
Each language in the data set has three files with one sentence on each line. One file draws from Twitter, one from blogs, and the other from news sites. All three have between 30 and 40 million words in them. Here are the individual summaries for the English files, which we will use for this project:

Source | Lines | Words (total)
--- | --- | ---
Twitter | 2,360,148 | 30,359,804
News | 1,010,242 | 34,365,936
Blogs | 899,288 | 37,334,114

Because these files are so large (~200 MB each) and because of the limited resources on my machine, we are going to work with a representative sample of the data. The sample files were created with the following Linux commands:

```
shuf -n 1000 en_US.news.txt > sample.news.txt
shuf -n 1000 en_US.blogs.txt > sample.blogs.txt
shuf -n 3000 en_US.twitter.txt > sample.twitter.txt
```

We sample 3000 lines from Twitter in order to get roughly the same word count as news and blogs have, since their line lengths are longer. The sampled files are included in this repository for reproducibility. From that, we remove numbers and punctuation, transform all the words to lowercase, and remove stopwords (simple words like "the", "too", "not", etc.). Here are the statistics for these sampled files:

```{r}
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)
library(cowplot)

con <- file("data/en_US/sample.twitter.txt", "r")
twitterLines <- readLines(con)
close(con)

con <- file("data/en_US/sample.news.txt", "r")
newsLines <- readLines(con)
close(con)

con <- file("data/en_US/sample.blogs.txt", "r")
blogLines <- readLines(con)
close(con)

twitter.corpus <- Corpus(VectorSource(twitterLines))
twitter.corpus <- tm_map(twitter.corpus, content_transformer(tolower))
twitter.corpus <- tm_map(twitter.corpus, removePunctuation)
twitter.corpus <- tm_map(twitter.corpus, removeNumbers)
twitter.corpus <- tm_map(twitter.corpus, function(x) removeWords(x, stopwords("english")))
twitter.tdm <- TermDocumentMatrix(twitter.corpus)
twitter.m <- as.matrix(twitter.tdm)
twitter.v <- sort(rowSums(twitter.m),decreasing=TRUE)
twitter.d <- data.frame(word = names(twitter.v),freq=twitter.v)

news.corpus <- Corpus(VectorSource(newsLines))
news.corpus <- tm_map(news.corpus, content_transformer(tolower))
news.corpus <- tm_map(news.corpus, removePunctuation)
news.corpus <- tm_map(news.corpus, removeNumbers)
news.corpus <- tm_map(news.corpus, function(x) removeWords(x, stopwords("english")))
news.tdm <- TermDocumentMatrix(news.corpus)
news.m <- as.matrix(news.tdm)
news.v <- sort(rowSums(news.m),decreasing=TRUE)
news.d <- data.frame(word = names(news.v),freq=news.v)

blog.corpus <- Corpus(VectorSource(blogLines))
blog.corpus <- tm_map(blog.corpus, content_transformer(tolower))
blog.corpus <- tm_map(blog.corpus, removePunctuation)
blog.corpus <- tm_map(blog.corpus, removeNumbers)
blog.corpus <- tm_map(blog.corpus, function(x) removeWords(x, stopwords("english")))
blog.tdm <- TermDocumentMatrix(blog.corpus)
blog.m <- as.matrix(blog.tdm)
blog.v <- sort(rowSums(blog.m),decreasing=TRUE)
blog.d <- data.frame(word = names(blog.v),freq=blog.v)
```

Source | Lines | Words (total) | Words (unique)
--- | --- | --- | ---
Twitter | `r length(twitter.corpus)` | `r sum(twitter.d$freq)` | `r length(twitter.d$word)`
News | `r length(news.corpus)` | `r sum(news.v)` | `r length(news.d$word)`
Blogs | `r length(blog.corpus)` | `r sum(blog.v)` | `r length(blog.d$word)`

# Word length

It may be interesting to examine word lengths:

```{r include=TRUE}
twitter.wordlen <- sapply(as.character(twitter.d$word), nchar)
news.wordlen <- sapply(as.character(news.d$word), nchar)
blog.wordlen <- sapply(as.character(blog.d$word), nchar)

plot_grid(qplot(twitter.wordlen, geom="histogram", xlab="Twitter word length"),
          qplot(news.wordlen, geom="histogram", xlab="News word length"),
          qplot(blog.wordlen, geom="histogram", xlab="Blogs word length"),
          ncol=1,
          align="v")
```

Source | Median | Mean | Max
--- | --- | --- | ---
Twitter | `r median(twitter.wordlen)` | `r mean(twitter.wordlen)` | `r max(twitter.wordlen)`
News | `r median(news.wordlen)` | `r mean(news.wordlen)` | `r max(news.wordlen)`
Blogs | `r median(blog.wordlen)` | `r mean(blog.wordlen)` | `r max(blog.wordlen)`

# How many words do we need to model?
In the previous section, the extremely long words in all three datasets tended to be URLs, which happened because of the way we removed punctuation and numbers. In our model, we will want to exclude these words. In fact, we can calculate how many words (sorted descending by frequency) we'd need from each set to represent 50% or 80% of all the words in that set:

```{r}
calcPercentageSubset <- function(freqs, percentage) {
  runningSum <- 0
  stopVal <- sum(freqs) * percentage
  for (i in 1:length(freqs)) {
    runningSum <- runningSum + freqs[i]
    if (runningSum >= stopVal) {
      break
    }
  }
  return(i)
}

twitter.perc50 <- calcPercentageSubset(twitter.d$freq, 0.5)
news.perc50 <- calcPercentageSubset(news.d$freq, 0.5)
blog.perc50 <- calcPercentageSubset(blog.d$freq, 0.5)
twitter.perc80 <- calcPercentageSubset(twitter.d$freq, 0.8)
news.perc80 <- calcPercentageSubset(news.d$freq, 0.8)
blog.perc80 <- calcPercentageSubset(blog.d$freq, 0.8)
```

Source | 50% of all words | 80% of all words
--- | --- | ---
Twitter | `r twitter.perc50` | `r twitter.perc80`
News | `r news.perc50` | `r news.perc80`
Blogs | `r blog.perc50` | `r blog.perc80`

We'll probably want to use only the top certain percentage of words in building the models.

# Visualizing the most frequent words

Now for fun we can do a wordcloud with the top 100 words in each group in decreasing frequency.

First, Twitter:

```{r include=TRUE}
pal2 <- brewer.pal(8,"Dark2")

suppressMessages(wordcloud(twitter.d$word, twitter.d$freq, max.words=100, random.order=FALSE, colors=pal2))
```

News sites:

```{r include=TRUE}
suppressMessages(wordcloud(news.d$word, news.d$freq, max.words=100, random.order=FALSE, colors=pal2))
```

Blogs:

```{r include=TRUE}
suppressMessages(wordcloud(blog.d$word, blog.d$freq, max.words=100, random.order=FALSE, colors=pal2))
```

# Conclusion

We've learned that the blog, news, and Twitter corpora are different in their composition and vocabulary complexity. Using all three will make our model more robust, and may give us some useful context-sensitive information we can apply.

Now that we know a bit about the characteristics of these data, we can start building some interesting models!